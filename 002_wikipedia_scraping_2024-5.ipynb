{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5865c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup, Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "701ea9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96527bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_h2(element):\n",
    "    current_element = element\n",
    "    while True:\n",
    "        current_element = current_element.previous_element\n",
    "        if current_element is None:\n",
    "            return None\n",
    "        if isinstance(current_element, Tag) and current_element.name == 'h2':\n",
    "            return current_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7799ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_matches(url):\n",
    "    \n",
    "    filename = url.split('/')[-1] + \".json\"\n",
    "    \n",
    "    print(f\"downloading {filename}\")\n",
    "    \n",
    "    zapasy = []\n",
    "    itemprops = ['homeTeam','awayTeam','location']\n",
    "    r = requests.get(url, timeout=20, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    fotbalboxy = soup.find_all(class_='footballbox')\n",
    "    for f in fotbalboxy:\n",
    "        zapas = {}\n",
    "        zapas['competition'] = urllib.parse.unquote(url.split('/')[-1].replace(\"_\",\" \"))\n",
    "        zapas['date'] = f.find(class_='fdate').text.split(\"(\")[0].split('[')[0].replace(\"\\xa0\",\" \").strip()\n",
    "        if f.find(class_='fdate').find(\"sup\"):\n",
    "            zapas['date'] = zapas['date'].replace(f.find(class_='fdate').find(\"sup\").text,\"\")\n",
    "        zapas['score'] = f.find(class_='fscore').text.replace(\"â€“\",\"-\")\n",
    "        if f.find(class_='fscore').find('sup'):\n",
    "            zapas['score'] = zapas['score'][:-len(f.find(class_='fscore').find('sup').text)]\n",
    "        for i in itemprops:\n",
    "            try:\n",
    "                zapas[i] = f.find(itemprop=i).text.split('[')[0].strip()\n",
    "                if i != itemprops[-1]:\n",
    "                    zapas[f\"{i}Country\"] = f.find(itemprop=i).find(class_='flagicon').find('img')['alt']\n",
    "                    urls = f.find(itemprop=i).find(itemprop='name').find_all('a')\n",
    "                    for u in urls:\n",
    "                        if not u.find_all('img'):\n",
    "                            zapas[f\"{i}Url\"] = u['href'].replace('/wiki/','')\n",
    "            except:\n",
    "                print(i)\n",
    "        zapas['phase'] = find_last_h2(f).text.split('[')[0].strip()\n",
    "        for d in f.find_all('div'):\n",
    "            if \"Attendance\" in d.text:\n",
    "                zapas['attendance'] = d.text.replace('Attendance: ','')\n",
    "                if d.find('sup'):\n",
    "                    zapas['attendance'] = zapas['attendance'][:-len(d.find('sup').text)]\n",
    "        zapasy.append(zapas)\n",
    "\n",
    "    with open(os.path.join('data_raw',filename), \"w+\", encoding=\"utf-8\") as soubor:\n",
    "        soubor.write(json.dumps(zapasy))\n",
    "\n",
    "    return(zapasy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26a8bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "download = {\n",
    "    'UCL': [\n",
    "        'https://en.wikipedia.org/wiki/2024%E2%80%9325_UEFA_Champions_League_qualifying_phase_and_play-off_round',\n",
    "        'https://en.wikipedia.org/wiki/2024%E2%80%9325_UEFA_Champions_League',\n",
    "        \n",
    "    ],\n",
    "    'EL': [\n",
    "        'https://en.wikipedia.org/wiki/2024%E2%80%9325_UEFA_Europa_League_qualifying_phase_and_play-off_round'],\n",
    "    'UECL': [\n",
    "        'https://en.wikipedia.org/wiki/2024%E2%80%9325_UEFA_Conference_League_qualifying_phase_and_play-off_round_(Main_Path)',\n",
    "        'https://en.wikipedia.org/wiki/2024%E2%80%9325_UEFA_Conference_League_qualifying_phase_and_play-off_round_(Champions_Path)'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac5cc72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 2024%E2%80%9325_UEFA_Champions_League_qualifying_phase_and_play-off_round.json\n",
      "downloading 2024%E2%80%9325_UEFA_Champions_League.json\n",
      "downloading 2024%E2%80%9325_UEFA_Europa_League_qualifying_phase_and_play-off_round.json\n",
      "downloading 2024%E2%80%9325_UEFA_Conference_League_qualifying_phase_and_play-off_round_(Main_Path).json\n",
      "downloading 2024%E2%80%9325_UEFA_Conference_League_qualifying_phase_and_play-off_round_(Champions_Path).json\n"
     ]
    }
   ],
   "source": [
    "for competition, pages in download.items():\n",
    "    for p in pages:\n",
    "        scrape_matches(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
